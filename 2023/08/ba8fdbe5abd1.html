<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="鹤染">





<title>Raft | Starry54</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Starry54&#39; Notes</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Starry54&#39; Notes</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Raft</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">鹤染</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">August 5, 2023&nbsp;&nbsp;1:16:38</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/MIT-6-824/">MIT 6.824</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="寻找一种易于理解的一致性算法（扩展版）"><a href="#寻找一种易于理解的一致性算法（扩展版）" class="headerlink" title="寻找一种易于理解的一致性算法（扩展版）"></a>寻找一种易于理解的一致性算法（扩展版）</h1><h2 id="0-Abstract-概要"><a href="#0-Abstract-概要" class="headerlink" title="0 Abstract 概要"></a>0 Abstract 概要</h2><p>Raft是一个用于管理复制日志的共识算法。 它提供了和Paxos算法同样的结果和性能，但是它的结构和Paxos不同；这使得Raft比Paxos更易于理解和更易于构建实际的系统。 为了加强可理解性，Raft将一致性算法分解为多个模块，包括leader选举，log复制，安全性等，它通过更强的一致性来减少必须考虑的状态的数量。 一项用研究表明对于学生而言，Raft算法比Paxos更易于学习。 Raft算法还包括一种全新的集群成员变更机制，利用重叠的主体来保证安全性。</p>
<h2 id="1-Introduction-介绍"><a href="#1-Introduction-介绍" class="headerlink" title="1 Introduction 介绍"></a>1 Introduction 介绍</h2><p>共识算法允许一组机器作为一个整体共工作(即使其中的一些出现了故障)。 正因如此，共识算法在构建可靠大规模软件系统中扮演重要的角色。 Paxos在过去十年中在共识算法领域处于统治地位：绝大部分的实现都是基于或者受到Paxos的影响，并且Paxos也成为教学中关于共识问题的主要例子。</p>
<p>不幸的是，尽管已经通过无数次尝试去降低它的复杂性，Paxos仍然相当难以理解。 而且，Paxos的结构需要复杂的修改才能支持实际的系统。 因此，无论是学术界还是工业界都对此感到棘手。</p>
<p>在和Paxos的斗争中，我们开始寻找寻找新的共识算法，可以为学术界和工业界提供更好的基础。 同Paxos不同，我们的首要目标是可理解性：我们能否在实际系统中定义一个共识算法，并且比Paxos更易于学习？ 此外，我们希望这个算法能够促进直觉的发展，这对于系统构建者很重要。 重要的不仅仅是算法可以运作，而是能够理解为什么可以它可以生效。</p>
<p>Raft算法就是这些工作的结果。在设计Raft时，我们使用了一些特定的技巧来提高它的可以理解性，包括模块分解(Raft主要分为Leader选举，Log复制，安全三个模块)，减少状态(相对于Paxos，Raft减少了非确定性和服务器处于非一致性的状态)。 一份针对两所大学的43名学生的研究表明，Raft相比Paxos更易于学习，这些学生学习两种算法后，33名能回答关于Raft的问题，效果优于Paxos。</p>
<p>Raft算法在许多方面和现在的共识算法相似(主要是Oki和Liskov的Viewstamped Replication)，但是它也有一些独特的特性：</p>
<ul>
<li><strong>强Leader</strong>：和其他的共识算法相比，Raft使用一种更强的领导力形式。例如，日志条目只从Leader发送到其他的服务器。这种方式简化了对于复制日志的管理，并使得Raft算法更易于理解。</li>
<li><strong>Leader选举</strong>：Raft使用一个随机计时器来选举Leader。这只是在所有共识算法必备的心跳机制上添加了一些内容，可以更方便快捷地解决冲突。</li>
<li><strong>成员关系调整</strong>：Raft通过一种全新的联合共识算法来解决集群中的成员变换问题，在这种方式下，处理调整过程中的两种不同配置的集群得到大部分机器会有重叠，这就使得集群在成员变更时也能继续运作。</li>
</ul>
<p>我们认为Raft比Paxos和其他的共识算法出色，无论是在教学中还是作为实践的基石。它比其他算法更加简单和易于理解；它的算法描述足以去实现一个真实的系统；他有好多开源的实现，并且应用于多个企业；它的安全性已经被正式声明和证实；它的效率也和其他的算法相当。</p>
<p>接下来，论文会介绍以下内容：复制状态机问题，讨论Paxos的优缺点，讨论我们达成可理解性的方式，阐述Raft共识算法，评估Raft算法，以及一些相关的工作。</p>
<h2 id="2-Replicated-state-machines-复制状态机"><a href="#2-Replicated-state-machines-复制状态机" class="headerlink" title="2 Replicated state machines 复制状态机"></a>2 Replicated state machines 复制状态机</h2><p>共识算法是在复制状态机的背景下提出的。 在这种方式下，一组服务器的状态机产生相同状态的副本，并且在一些机器宕机的情况下也可以继续运行。</p>
<p>复制状态机在分布式系统中通常被用于解决很多容错性的问题。例如，一个大规模系统中通常有一个集群Leader，像GFS，HDFS，和RAMCloud，通常是使用一个独立的复制状态机去管理Leadler选举和存储配置信息(在Leader宕机时也需要存活)。例如Chubby和Zookeeper。</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3ba96462c1924892b3eaf504e669f11e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 1"></p>
<blockquote>
<p>Figure 1: 复制状态机的结构。共识算法管理一个复制的日志，其中包含来自客户端的状态机命令。状态机处理来自日志的相同命令序列，因此产生相同的输出。</p>
</blockquote>
<p>复制状态机通常是基于复制日志实现的，如图Figure 1。每个服务器存储一个包含一系列命令的日志，并按照日志的顺序执行。每个日志包含相同的指令(顺序也一致)，所以每个服务器都执行相同的指令序列。因为状态机都是确定的，每次都会得出相同的状态和输出序列。</p>
<p>保持复制日志的一致性是共识算法的任务。服务器上的共识模块接手客户端的命令，并且将它们添加到日志中。 它和其他的服务器上的共识模块通信，以保证每份日志最终都包含相同的请求序列(即使有的服务器发生了故障)。 一旦命令被正确的复制，每个服务器的状态机会按照日志顺序执行，然后给客户端返回输出。 因此，服务器集群就像是一个高可靠的状态机。</p>
<p>实际系统中的共识算法通常有如下的特征：</p>
<ul>
<li>安全性(绝不会返回错误的结果)：在非拜占庭错误的情况下，包括网络延迟，分区，丢包，重复，乱序等，可以保证正确。</li>
<li>功能性：只要有绝大多数的机器可以运行，并能够互相通信、和客户端通信，就可以保证系统的可用性。因此，一般一个包含5个节点的集群可以容忍2个节点的错误。服务器被停止就认为是失败，之后可能从可靠存储的状态中恢复并重新加入集群。</li>
<li>一致性：它们不依赖时序来保证一致性，物理时钟错误或极端的消息延迟只有最坏情况下可能导致可用性问题。</li>
<li>通常情况下，一条命令可以尽快的在集群的大多数接种响应一轮RPC时完成。小部门较慢的节点并不会影响整体的性能。</li>
</ul>
<h2 id="3-What’s-wrong-with-Paxos-Paxos算法的缺点"><a href="#3-What’s-wrong-with-Paxos-Paxos算法的缺点" class="headerlink" title="3 What’s wrong with Paxos? Paxos算法的缺点"></a>3 What’s wrong with Paxos? Paxos算法的缺点</h2><p>在过去的10年中，Leslie Lamport的Paxos算法几乎成为了共识算法的代表：Paxo是课程教学中最普遍的算法，同时也是大多数共识算法的基础。 Paxos首先定义了一个能够达成单一决策一致的协议，比如单条的复制日志项。我们把这一子集称作单决策Paxos。然后Paxos通过组合多个这样的实力来促进一系列决策(例如log)。 Paxos保证安全性和有效性，同时支持集群成员的变化。它已经被证明在正常情况下都是有效的。</p>
<p>不幸的是，Paxos算法有两个明显的缺陷。 <strong>第一个问题</strong>就是Paxos相当难以理解。它的完整的解释及其不透明，即便付出了巨大的努力，也只有少数人成功理解了它。因此，人们多次尝试用更简单的术语来解释Paxos。 尽管这个解释都只关注了单决策子集，但仍然充满挑战。 在2012年的NSDI会议中，我们发现只有极少数人对Paxos感到舒适，即便是在一群经验丰富的研究者中。 我们也尝试去理解Paxos，知道我们读了很多简化的解释和设计了我们自己的协议后，我们才理解了Paxos，这个过程花费了近一年时间。</p>
<p>我们猜想Paxos算法的不透明性是由它选择单决策问题作为基础导致的。 单决策Paxos是晦涩的，微妙的：它划分成两种没有简单直观解释和无法独立理解的情形。因此，这导致了很难对为什么单决策Paxos算法有效建立起直观的感受。多决策Paxos的构成规则中增加了很多错综复杂的内容。我们相信，在多决策上能够达成共识的问题(一份日志，而非单一的记录)能够被分解为其他的更加直接和显然的方法。</p>
<p><strong>第二个问题</strong>就是Paxos没有提供一个良好的用于构建现实系统的基础。 一个原因是目前还没有被广泛接受的多决策问题的Paxos算法。Lamport的描述大部分都是关于单决策Paxos的；他简要描述了实施多决策Paxos算法的可能方式，但是缺少了很多细节。当然，也有很多扩充和优化Paxos的尝试，但是它们各不相同。例如Chubby这样的系统实现了一个类Paxos算法，但是大部分的细节没有被公开。</p>
<p>而且，Paxos算法在构建现实系统方面并不理想；这是单决策分解所导致的。例如，独立选择日志条目的集合，然后将它们合并成一个日志序列并没有任何好处，只会徒增复杂度。围绕日志设计系统会更加简单高效，新的日志按照顺序依次附加即可。另一个问题是，Paxos使用了一种点对点的对等方式作为它的核心(尽管它最终建议采用一种弱Leader的方式来优化性能)。这在只有一个决策会被制定的简化模型中是有效的，但是在现实系统中很少使用这样的方式。如果有一系列的决策需要被制定，首先选举一个Leader，然后让它去协调决策会更加有效和快速。</p>
<p>因此，实际的系统总很少有和Paxos相似的场景。 每一种实现都是从Paxos开始起步，然后发现了很多实现方面的难题，接着开发一种和Paxos结构不同的算法。 这样是非常耗时和容易出错的，并且理解Paxos算法的难度加剧了这个问题。 Paxos算法在理论上被证明是可行的，但是现实的系统和Paxos的差异是如此巨大，以至于这些证明价值有限。下面来自Chubby的实现者的评价很经典：</p>
<blockquote>
<p>在Paxos算法的描述和现实系统的需求中间存在着巨大的鸿沟。…最终的系统将会建立在一个没有严格证明的基础上。</p>
</blockquote>
<p>因此，我们任务Paxos算法既没有提供一个良好的基础给现实系统，也没有在教学方面有很好的作用。考虑到共识问题在大规模软件集群系统中的照耀性，我们决定尝试我们是否可以设计一个更优质的，可以替代Paxos的共识算法，Raft即使这次尝试的结果。</p>
<h2 id="4-Designing-for-understandability-为了可理解性的设计"><a href="#4-Designing-for-understandability-为了可理解性的设计" class="headerlink" title="4 Designing for understandability 为了可理解性的设计"></a>4 Designing for understandability 为了可理解性的设计</h2><p>设计Raft时我们有如下目标：</p>
<ol>
<li>必须为系统构建提供一个完整和实用的基础，这样可以大大减少开发者的设计工作。</li>
<li>必须在任何情况下都是安全的，必须在大多数情况下都是可用的，并且对于大部分操作都是高效的。</li>
<li>必须能够让广大受众能容易地理解算法。(最重要的，最大的挑战)</li>
<li>必能让人形成直观的认识，这样系统构建者才能在现实中进行必要的拓展。</li>
</ol>
<p>在设计Raft算法时，我们需要在多个方案中进行抉择。 在这种情况下，我们基于可理解性原则评估各个方案：解释每种备选方案有多难(例如，其状态空间有多复杂，是否有微妙的影响？)以及读者完全理解该方法机器含义的难易程度。</p>
<p>我们意识到这种分析具有高度的主观性；尽管如此，我们使用了两种普遍适用的技术来解决这个问题。 首先就是众所周知的问题分解：我们尽可能地将问题拆分成几个相对独立的，可以被解决的，可解释的，可理解的子问题。例如Raft算法被我们分成Leader选举，Log复制，安全性和成员变更几个部分。</p>
<p>第二个方法就是通过减少需要考虑的状态数来简化状态空间，使得系统更加连贯并且尽可能消除不确定性。 特别的，所有的日志是不允许有空洞的，并且Raft限制了日志之间变不一致的可能。尽管大多数情况下我们都尝试去消除不确定性，但是也有一些场景下不确定性可以帮助我们理解。 尤其是，随机方法引入了非确定性，但它们倾向于以类似的方式处理所有的选择，从而减少状态空间。我们使用随机化方法来简化Raft的Leader选举。</p>
<h2 id="5-The-Raft-consensus-algorithm-Raft-共识算法"><a href="#5-The-Raft-consensus-algorithm-Raft-共识算法" class="headerlink" title="5 The Raft consensus algorithm Raft 共识算法"></a>5 The Raft consensus algorithm Raft 共识算法</h2><p>Raft是一种用来管理第2章中描述的Log复制算法。Figure 2简单概括了这个算法，Figure 3列出了算法的关键属性；这些图中的元素将会在本部分进行逐一讨论。</p>
<p>Raft通过选举一个杰出的Leader，然后赋予Leader全部的管理复制日志的职责，从而达成共识。 Leader从客户端接手日志条目(Log entries)，把日志条目复制到其他服务器上，并且通知其他的服务器可以安全的将条目应用到状态机中。 拥有一个Leader大大简化了对复制日志的管理。例如，Leader可以决定新的条目存放的位置而不需要和其他的服务器协商，并且数据流都从Leader指向其他的服务器。 一个Leader可能会发生故障，或者与其他的服务器断开连接，这种情况下需要选举一个新的Leader。</p>
<p>通过Leader 的方式，Raft将共识问题拆分成三个相对独立的子问题，下面将会一一讨论：</p>
<ul>
<li>Leader选举：当现存的Leader故障时必须选举出新的Leader。(5.2)</li>
<li>日志复制：Leader必须从客户端接受日志条目，然后复制到集群的其他节点中，并强制其他节点和自己保持一致。(5.3)</li>
<li>安全性：Raft中安全性的关键在于Figure 3中的状态机安全：如果有任何的节点已经在其状态集中应用某条日志，则其他服务器不得对同一个日志索引应用不同的命令。6.4中将描述Raft如何确保这一特性：该方案涉及到对5.2中选举机制的额外限制。</li>
</ul>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a961b1b4deda4b5dbd77bb810196c055~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 2"></p>
<blockquote>
<p>Figure 2：Raft 共识算法（不包括成员变更和日志压缩）的概要。左上角方框中的服务器行为被描述为一组独立重复触发的规则。章节编号（如§5.2）表示讨论特定功能的位置。正式规范[31]更详细地描述了算法。</p>
</blockquote>
<h3 id="State-状态"><a href="#State-状态" class="headerlink" title="State 状态"></a>State 状态</h3><h4 id="Persistent-state-on-all-servers-所有的服务器上的持续性状态"><a href="#Persistent-state-on-all-servers-所有的服务器上的持续性状态" class="headerlink" title="Persistent state on all servers 所有的服务器上的持续性状态"></a>Persistent state on all servers 所有的服务器上的持续性状态</h4><p>响应RPCs之前已经更新到稳定存储了</p>
<table>
<thead>
<tr>
<th>State</th>
<th>Content</th>
</tr>
</thead>
<tbody><tr>
<td>currentTerm</td>
<td>服务器已知的最新的任期(初始为0，自增)</td>
</tr>
<tr>
<td>votedFor</td>
<td>当前任期内收到选票候选者Id(如果没有则为null)</td>
</tr>
<tr>
<td>log[]</td>
<td>日志条目；每个条目包括了状态机的命令，以及Leader接收到该条目时的任期(初始为1)</td>
</tr>
</tbody></table>
<h4 id="Volatile-state-on-all-servers-所有的服务器上的易失性状态"><a href="#Volatile-state-on-all-servers-所有的服务器上的易失性状态" class="headerlink" title="Volatile state on all servers 所有的服务器上的易失性状态"></a>Volatile state on all servers 所有的服务器上的易失性状态</h4><table>
<thead>
<tr>
<th>State</th>
<th>Content</th>
</tr>
</thead>
<tbody><tr>
<td>commitIndex</td>
<td>已提交的最大日志索引(初始化为0，单调自增)</td>
</tr>
<tr>
<td>lastApplied</td>
<td>作用于状态机的最大日志条目（初始化为0，单调自增）</td>
</tr>
</tbody></table>
<h4 id="Volatile-state-on-leaders-Leader节点的易失性状态"><a href="#Volatile-state-on-leaders-Leader节点的易失性状态" class="headerlink" title="Volatile state on leaders Leader节点的易失性状态"></a>Volatile state on leaders Leader节点的易失性状态</h4><p>在选举后重新初始化</p>
<table>
<thead>
<tr>
<th>State</th>
<th>Content</th>
</tr>
</thead>
<tbody><tr>
<td>nextIndex[]</td>
<td>对于每个服务器，下一条发送的日志条目索引(初始化微Leader的最后一条日志的索引+1)</td>
</tr>
<tr>
<td>matchIndex[]</td>
<td>对于每个服务器，已知的复制完成的最大日志条目索引(初始化为0，单调自增)</td>
</tr>
</tbody></table>
<h3 id="AppendEntries-RPC-附加条目RPC"><a href="#AppendEntries-RPC-附加条目RPC" class="headerlink" title="AppendEntries RPC 附加条目RPC"></a>AppendEntries RPC 附加条目RPC</h3><p>由Leader发起，用于复制日志条目，同时可以用于维持心跳</p>
<h4 id="Arguments-参数"><a href="#Arguments-参数" class="headerlink" title="Arguments 参数"></a>Arguments 参数</h4><ul>
<li>term：Leader的任期</li>
<li>leaderId：Follower根据ID将客户端的请求重定向到Leader节点</li>
<li>prevLogIndex：紧邻新日志条目的前继条目索引</li>
<li>preLogTerm：紧邻新日志条目的前继条目任期</li>
<li>entries[]：日志条目(心跳条目为空)</li>
<li>leaderCommit：Leader的commitIndex</li>
</ul>
<h4 id="Results-返回值"><a href="#Results-返回值" class="headerlink" title="Results 返回值"></a>Results 返回值</h4><ul>
<li>term：当前任期</li>
<li>success：如果Follower的条目和参数中的匹配，则响应成功</li>
</ul>
<h4 id="Receiver-implementation-接受者的实现"><a href="#Receiver-implementation-接受者的实现" class="headerlink" title="Receiver implementation 接受者的实现"></a>Receiver implementation 接受者的实现</h4><ol>
<li>如果 term &lt; currentTerm，返回false</li>
<li>如果日志不匹配prevLogIndex和prevLogTerm，返回false</li>
<li>如果一个已有条目和新条目冲突，删除已有条目</li>
<li>追加日志中不存在的新条目</li>
<li>如果leaderCommit &gt; commitIndex，将commitIndex改成<code>min(leaderCommit, index of last new entry)</code></li>
</ol>
<h3 id="RequestVote-RPC-请求投票RPC"><a href="#RequestVote-RPC-请求投票RPC" class="headerlink" title="RequestVote RPC 请求投票RPC"></a>RequestVote RPC 请求投票RPC</h3><p>由candidates发起用于收集选票</p>
<h4 id="Arguments-参数-1"><a href="#Arguments-参数-1" class="headerlink" title="Arguments 参数"></a>Arguments 参数</h4><ul>
<li>term：candidate的任期</li>
<li>candidateId：请求投票的服务器Id</li>
<li>lastLogIndex：candidate的最后一条日志条目索引</li>
<li>lastLogTerm：candidate的最后一条日志条目的任期</li>
</ul>
<h4 id="Results-返回值-1"><a href="#Results-返回值-1" class="headerlink" title="Results 返回值"></a>Results 返回值</h4><ul>
<li>term：当前任期，以便candidate更新任期号</li>
<li>voteGranted：若投票给该节点，则为真</li>
</ul>
<h4 id="Receiver-implementation-接受者实现"><a href="#Receiver-implementation-接受者实现" class="headerlink" title="Receiver implementation 接受者实现"></a>Receiver implementation 接受者实现</h4><ol>
<li>如果term &lt; currentTerm，返回false</li>
<li>如果votedFor为空，或者为candidateId，且candidate的日志和自己一样新，那么投票给它</li>
</ol>
<h3 id="Rules-for-Servers-所有服务器需要遵循的规则"><a href="#Rules-for-Servers-所有服务器需要遵循的规则" class="headerlink" title="Rules for Servers 所有服务器需要遵循的规则"></a>Rules for Servers 所有服务器需要遵循的规则</h3><h4 id="所有服务器"><a href="#所有服务器" class="headerlink" title="所有服务器"></a>所有服务器</h4><ul>
<li>如果commitIndex &gt; lastApplied，则lastApplied递增，并将log[lastApplied]作用于状态机</li>
<li>如果接受的RPC请求&#x2F;响应中，任期号T &gt; currentTerm，则另currentTerm &#x3D; T，并切换为Follower状态</li>
</ul>
<h4 id="Follower"><a href="#Follower" class="headerlink" title="Follower"></a>Follower</h4><ul>
<li>响应来自Candidates和Leader的请求</li>
<li>如果在超过选举时间的情况下没有收到当前Leader的心跳，或者给某个Candidate投票，那么自己就成为Candidate</li>
</ul>
<h4 id="Candidate"><a href="#Candidate" class="headerlink" title="Candidate"></a>Candidate</h4><ul>
<li>转变为Candidate后就立刻发起选举过程<ul>
<li>自增任期号</li>
<li>给自己投票</li>
<li>重置选举超时计时器</li>
<li>发送RequestVoteRPC给其他服务器</li>
</ul>
</li>
<li>如果接受大部分选票，成为Leader</li>
<li>如果收到新Leader的AppendEntriesRPC，则变为Follower</li>
<li>如果选举过程超时，发起新一轮选举</li>
</ul>
<h4 id="Leader"><a href="#Leader" class="headerlink" title="Leader"></a>Leader</h4><ul>
<li>一旦成为Leader：先发送心跳给其他节点，并且间隔一段时间就会重新发送</li>
<li>如果接收到客户端的请求，附加条目到本地日志，在该条目应用到状态机后，响应客户端</li>
<li>对于一个Follower，如果lastLogIndex &gt; nextIndex，则发送从nextIndex开始的所有与条目<ul>
<li>成功：更新Follower的nextIndex和matchIndex</li>
<li>失败：减小nextIndex并重试</li>
</ul>
</li>
<li>假设存在N &gt; commitIndex，使得大多数的matchIndex[i] &gt;&#x3D; N，log[N].term &#x3D;&#x3D; currentTerm，则commitIndex &#x3D; N</li>
</ul>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/466e7e0a399b4c5c9c0dfee548076162~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 3"></p>
<blockquote>
<p>Figure 3：Raft 保证这些属性在任何时候都是正确的。章节编号表示讨论每个属性的位置。</p>
</blockquote>
<p>在展示共识算法之后，这一章会讨论一些可用性相关的问题和系统中计时的角色。</p>
<h3 id="5-1-Raft-basics-Raft-基础"><a href="#5-1-Raft-basics-Raft-基础" class="headerlink" title="5.1 Raft basics Raft 基础"></a>5.1 Raft basics Raft 基础</h3><p>一个Raft集群包括若干个服务器节点；5个是一个典型的例子，这允许系统中有两个节点失效。在任意时刻，每一个服务器都处于以下三个状态之一：Leader，Follower或者Candidate。 在正常运行的情况下，系统中只有一个Leader，其他节点都是Follower。 Follower都是被动的：他们不会发出任何请求，只能单纯的响应来自Leader和Candidate的请求。 Leader会处理所有的客户端请求(如果客户端和Follower通信，那么Follower节点会将请求转发给Leader)。 第三种状态是Candidate，用于选举新的Leader，详见5.2。 Figure 4展示了这些状态及其转换；下面将介绍这些转换。</p>
<p>Raft将时间分割为任意长度的任期，如图Figure 5。 任期用连续的整数标。每段任期都从一次选举开始，一个或者多个Candidate尝试成为Leader。 如果一个Candidate赢得了选举，那么他就会在接下来的任期内担任Leader的角色。 某些时候，一次选举会出现选票的瓜分。这种情况下，这段任期将会没有Leader；一段新的任期很快就会开始(一次新的选举)。 Raft保证了在给定的任期内最多只有一个Leader。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fbab537a4dc74fe0a50dab7710e12dda~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 4"></p>
<blockquote>
<p>Figure 4：服务器状态。Followers只响应来自其他服务器的请求。如果一个Follower没有收到任何通信，它将成为Candidate并发起选举。获得整个集群多数票的Candidate将成为新的Leader。Leader通常运行到失败为止</p>
</blockquote>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5517725e2c0c4f03a7d80d8ac9a4215a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 5"></p>
<blockquote>
<p>Figure 5： 时间被划分为几个任期，每个任期从选举开始。选举成功后，由一个Leader管理集群，直到任期结束。有些选举失败，在这种情况下，任期结束时没有选择Leader。在不同的服务器上，可以观察到任期之间的不同时间转换。</p>
</blockquote>
<p>不同的服务器节点可能在不同的时间观察到任期之间的转换。在某些情况下，一个服务器可能察觉到一次选举，甚至察觉不到整个任期。 任期在Raft算法中充当逻辑时钟的角色，任期使得服务器可以检测一些过期的信息，例如过期的Leader。 每个服务器节点存储一个当前的任期号，该编号随时间自增。当服务器之间通信时它们会交换当前的任期号；如狗哦一个服务器的当前任期号小于另一个，那么它会更新自己的任期号为另一个的值。 如果一个Candidate或者Leader发现自己的任期号过期了，它会立即变为Follower的状态。 如果一个节点接收到一个包含过期任期号的请求，它会拒绝这个请求。</p>
<p>Raft算法中服务器通过RPC进行通信，并且基本的共识算法只需要两种RPC。 请求投票RPC(RequestVote RPCs)由Candidates在选举期间发起，附加条目RPC(AppendEntries RPCs)由Leader发起，用于复制日志或提供一种心跳形式。第七节为了在服务器之间传输快照，新增了第三种RPC。当服务器没有即使收到RPC的响应时，会进行重试，并且它们能够并行的发起RPC来获取最佳的性能。</p>
<h3 id="5-2-Leader-election-领导人选举"><a href="#5-2-Leader-election-领导人选举" class="headerlink" title="5.2 Leader election 领导人选举"></a>5.2 Leader election 领导人选举</h3><p>Raft采用一种心跳机制来触发Leader选举。 当服务器启动时，它们都是Follower。只要能从Leader或者Candidate接收到有效的RPC，一个节点就会一直保持Follower的状态。 Leader周期性的向所有Follower发送心跳包(不包含日志项内容的AppendEntries RPCs)来维持自己的权威。 如果一个Follower在一段时间内没有收到任何消息，即选举超时，它就会任务系统中没有可用的Leader并且会发起新的选举以选举新的Leader。</p>
<p>要开始一次选举过程，跟随者要自增当前任期号并且转变为Candidate状态。 接着它会并行的向集群中的其他服务器节点发送RequestVote RPCs来给自己投票， Candidate的状态一直保持，直到以下三件事之一发生：(a)它自己赢得了这次选举，(b)其他的服务器成为了Leader，(c)一段时间之后没有一个获胜者。这些结果分别会在下面讨论。</p>
<p>当一个Candidate获得了整个集群的大多数服务器节点的针对同一个任期号的选票，那么它就赢得了选举。每个服务器最多会针对一个任期号投出一张选票，按照First-come-first-serve的原则(5.4中增加一点额外的限制)。 要求大多数选票的规则确保了最多只有一个Candidate会赢得此次选举(Figure 3中要求的安全性)。 一旦一个Candidate赢得了选举，它就会成为Leader。然后它会向其他的服务器节点发送心跳来建立自己的权威，并且阻止发起新的选举。</p>
<p>在等待投票时，一个Candidate可能会从其他的服务器收到声明它是Leader的AppendEntries RPC。 如果这个Leader的任期号(会包含在这次RPC中)不小于当前Candidate的任期号，那么Candidate就会承认Leader的合法性，并且恢复Follower的状态。 如果此次RPC中的任期号小于自己，那么Candidate就会拒绝这次RPC并且继续保持Candidate的状态。</p>
<p>第三种可能的结果就是Candidate既没有赢得选举，也没有输：当多个Follower同时成为Candidate，那么投票可能会被瓜分以至于没有Candidate能获得大部分节点的选票。 当这种情况发生时，每个Candidate都会超时，然后通过自增任期号来开启一轮新的选举。 然而，选票可能会被无限瓜分，如果没有其他的附加机制的话。</p>
<p>Raft算法通过随机选举超时的方式来确保很少发生选票瓜分的情况，即使出现了也能很快解决。为了阻止选票被瓜分，选举超时时间是从一个固定的区间随机选取的。 这样可能将服务器都分散，从而大多数情况下只有一个服务器会选举超时；它会赢得选举并且在其他的服务器超时之前发送心跳包。 同样的机制被用于选票瓜分的情况下。 每个Candidate在开始选举的时候重置一个随机的选举超时时间，然后再超时时间内等待选举的结果；这样减少在新的选举中新出现选票瓜分的可能性。9.3中表明这种方式可以快速选举出Leader。</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c3f794d5f34415984931500f7eba6c3~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 6"></p>
<blockquote>
<p>Figure 6：日志由按顺序编号的条目组成。每个条目包含创建时的术语（每个方框中的数字）和状态机的命令。如果一个条目可以安全地应用于状态机，那么这个条目就被认为是已提交的。</p>
</blockquote>
<p>选举就是一个例子，说明了可理解性是如何指导我们在不同的方案之间做出选择的。最初我们打算使用一种排名系统：每个Candidate都被赋予一个唯一排名，用于在Candidates中进行选择。 如果一个Candidate发现另一个Candidate有更高的排名，它将会恢复Follower的状态，这样高排名的Candidate能更容易赢得下次选举。 但是我们发现，这种方式在可用性方面产生了一些微妙的问题(如果高排名的服务器宕机了，那么低排名的服务器会超时并再次进入Candidate状态。而且如果这个行为发生的够快，可能会重置整个选举过程)。 我们针对整个算法做了多次调整，但是每次调整之后都会带来新问题。最终我们认为随机重试的方法更加清晰和易于理解。</p>
<h3 id="5-2-Log-replication-日志复制"><a href="#5-2-Log-replication-日志复制" class="headerlink" title="5.2 Log replication 日志复制"></a>5.2 Log replication 日志复制</h3><p>一旦一个Leader被选举出来，它就开始为客户端提供服务。 客户端的每一条请求都包含一条被复制状态机执行的指令。 Leader把这条指令作为作为一条新的条目添加到日志中，然后并行地发起AppendEntries RPCs给其他的节点，让他们复制这条日志。当这条日志条目被安全地复制，Leader会应用这条日志条目到其状态集中，然后将执行的结果返回给客户端。 如果Follower崩溃或者运行缓慢，又或者网络丢包，Leader会不断的重试，直到所有的Follower都存储了所有的日志条目。 &#x2F;&#x2F; Figure 6</p>
<p>日志以Figure 6展示的方式组织。每一条日志条目存储一条状态机指令以及Leader传递的任期号。 日志中的任期号用来检查是否出现不一致的情况，同时也能保证Figure 3中的部分性质。 每一条日志条目同时也有一个整数索引值来表明它在日志中的位置。</p>
<p>Leader决定何时将日志条目应用到状态机是安全的，这样的条目被称为committed。Raft算法保证所有committed的条目都是持久化的并且最终都会被所有可用的状态机执行。 一旦创建日志条目的Leader在大多数的服务器上复制了该日志条目(如Figure 6中的entry 7)，该日志条目即为committed。 同时，Leader日志所有之前的条目，包括由之前的Leader创建的条目也会被commit。 5.4中会讨论某些当Leader变更之后应用这条规则的一些细节，同时它也表明这样的提交的定义是安全的。 Leader跟踪了最大的将会被提交的日志项的索引，并且索引值将会被包含在未来的所有AppendEntriesRPCs中，这样其他的服务器才能最终知道Leader的已提交位置。一旦Follower知道一条日志已被commit，那么它也会将这条日志应用到本地的状态机中(按照日志的顺序)。</p>
<p>我们设计了Raft的日志机制来维护不同的服务器日志之间的高度一致性。这样不仅简化了系统的行为也使其更可预测，同时这也是确保安全性的一个重要组成。 Raft维护着以下的特性，这些特征共同组成了Figure 3中的<strong>日志匹配特性</strong>：</p>
<ul>
<li>如果在不同的日志中两个条目拥有相同的索引和任期号，那么它们存储了相同的指令。</li>
<li>如果不同日志中的两个条目拥有相同的索引和任期号，那么它们之前的所有条目也相同。</li>
</ul>
<p>第一个特性来自这样一个事实，即在一个给定任期内，Leader最多创建一个具有给定日志索引的条目，并且该条目在日志中的位置永远不会改变。 第二个特征是由AppendEntriesRPC的一个简单地一致性检查所保证的。在发送AppendEntriesRPCs时，Leader会把新的条目前继条目的索引位置和任期号包含在日志内。如果Follower在它的日志中找不到包含着相同索引位置和任期号的条目，那么它就会拒绝接受新的条目。 一致性检查作为一个归纳步骤：一个初始为空的日志状态是满足日志匹配特征的，然后一致性检查在日志拓展时保证了日志匹配特征。因此每当AppendEntriesRPC返回成功，Leader直到Follower的日志一定是和自己保持相同。</p>
<p>在正常操作中，Leader的日志和Follower的日志保持着一致，所以AppendEntriesRPC的一致性检查不会失败。 但是，Leader崩溃时可能导致日志的不一致性(旧Leader没有完全复制所有日志条目)。这种不一致性可能会在Leader和Follower的一系列崩溃下加剧。Figure 7展示了Follower的日志和新的Leader不同的情形。Follower可能会丢失一些新Leader中存在的的日志，它也有可能存在有一些Leader中不存在的日志条目，或者两者都有。丢失或额外的日志条目可能持续多个任期。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bd4137fcf29a475ba98839f47ad66956~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 7"></p>
<p>在Raft算法中，Leader是通过强制Follower复制自己的日志来解决不一致的问题的。这意味着Follower日志中冲突的条目会被Leader的日志覆盖。5.4会阐述这样操作的安全性(通过额外的限制)。</p>
<p>要使得Follower的日志和自己保持一致，Leader必须找到它们达成一致的最后一条日志条目，然后删除Follower中之后的条目，并将自己的条目拷贝给它。 所有的这些操作都在AppendEntriesRPCs的一致性检查中完成。Leader会为每一个Follower维护一个nextIndex，它是Leader将发送给该Follower的下一条日志条目的索引。当一个Leader刚获得权力的时候，它会初始化所有的nextIndex的值为自己的最后一条日志的index+1(Figure 7中的11)。如果一个Follower的日志和Leader的不一致，那么在下一次的AppendEntriesRPC时一致性检查就会失败。 在被Follower拒绝之后，Leader会减小nextIndex的值并进行重试。最终这个nextIndex值会在某个位置和Follower的日志达成一致。当这种情况发生，AppendEntriesRPC就会成功，这是就会把Follower中冲突的条目移除，并拷贝Leader的日志。 一旦AppendEntriesRPC成功，那么Follower的日志就会和Leader保持一致，并且在任期中一直如此。</p>
<blockquote>
<p>如果需要的话，算法可以通过减少被拒绝的AppendEntriesRPC次数来进行优化。例如，当AppendEntriesRPC的请求被拒绝的时候，Follower可以返回冲突条目的任期号和该任期号对应的最小index。这样子，Leader可以减小nextIndex一次性越过该冲突任期的所有条目；这样就变成每个任期只需要一次AppendEntriesRPC。在实验中，我们对这种优化持怀疑态度，因为失败几乎不会发生，并且也几乎不能存在这么多不一致的日志。</p>
</blockquote>
<p>通过这种机制，Leader在获得权力时不同要通过任何特殊的措施来恢复日志的一致性。它只需要进行正常的操作，然后日志就能通过AppendEntriesRPC的一致性检查自动趋于一直。Leader从来不会覆写或者删除自己的条目(Figure 3的Leader的Append-Only特性)。</p>
<p>日志复制机制体现了第2节中的共识特征：Raft能够接受，复制，并应用新的条目只要大部分的服务器时正常的；在正常的情况下，新的条目可以再一轮RPCs中被复制给集群的大多数机器；并且单个运行缓慢的Follower不会影响整体的性能。</p>
<h3 id="5-4-Safety-安全性"><a href="#5-4-Safety-安全性" class="headerlink" title="5.4 Safety 安全性"></a>5.4 Safety 安全性</h3><p>前面的章节描述了Raft算法是如何选举Leader和复制日志条目的。 然而，到目前为止描述的机制并不能充分保证每一个状态机都按照相同的顺序执行相同的指令。例如：一个Follower可能会在Leader提交若干条目时进入不可用状态，然后这个Follower被选举为Leader并覆盖了这些条目；因此，不同的状态机可能会执行不同的指令序列。</p>
<p>这一节通过在Leader选举的时候增加一些限制来完善Raft算法。 这样的限制保证了任何的Leader对于给定的任期号，都拥有了之前的任期中所有committed 的条目(Figure 3中的Leader完整特性)。增加这一选举时，我们对于提交的规则也更加清晰。 最终，我们将会呈现对于Leader完整性特征的简要证明，以及说明该特性是如何保证复制状态机做出正确的行为的。</p>
<h4 id="5-4-1-Election-restriction-选举限制"><a href="#5-4-1-Election-restriction-选举限制" class="headerlink" title="5.4.1 Election restriction 选举限制"></a>5.4.1 Election restriction 选举限制</h4><p>在任何基于Leader的共识算法中，Leader都必须存储所有的committed的日志。 在某些共识算法中，例如Viewstamped Replication，即使一个节点一开始没有包含所有的committed日志，它也能被选举为Leader。 这些算法都包含了一些额外的机制来识别丢失的条目并将它们传送给新的Leader，要么是在选举阶段，要么是在之后很快地进行。 不幸的是，这导致了相当庞大的额外机制和复杂性。 Raft使用一种更简单地方式，它可以保证选举得到时候新的Leader拥有所有之前的任期中committed 的条目，而不需要传送这些条目给Leader。 这意味着日志条目的传送是单向的，只从Leader发送给Follower，并且Leader不会覆盖本身已有的日志。</p>
<p>Raft通过投票过程来阻止Candidate赢得选举，<strong>除非</strong>其日志已包含所有committed的条目。一个Candidate为了赢得选举必须和集群中的大部分节点通信，这意味着每一个committed日志在这些服务器节点中至少存在于一个节点上。 如果Candidate的日志和大多数的节点<strong>一样新</strong>，那么它一定持有了所有committed的条目。 RequestRPC实现了这种限制：RPC中包含了Candidate的日志信息，然后投票者会拒绝那些日志旧于自己的请求。</p>
<p>Raft通过比较两份日志中的最后一条日志的索引值和任期号来判断新旧。 如果两份日志的最后条目的任期号不同，那么任期号更大的日志更新。 如果两份日志最后条目的任期号相同，则日志较长的更新。</p>
<h4 id="5-4-2-Committing-entries-from-previous-terms-提交之前任期的条目"><a href="#5-4-2-Committing-entries-from-previous-terms-提交之前任期的条目" class="headerlink" title="5.4.2 Committing entries from previous terms 提交之前任期的条目"></a>5.4.2 Committing entries from previous terms 提交之前任期的条目</h4><p>如果5.3中描述的，Leader知道，一旦当期任期人的条目存储在大多数的节点时，该条目已被提交。 如果Leader在提交条目之前崩溃了，之后的Leader会完成日志的复制。 然而，一个Leader不能断定之前任期内的条目在存储到大多数节点时就一定committed了。Figure 8展示了一种情况，<strong>一条已经被存储到大多数节点的旧条目，依然可能被新Leader覆写</strong>。</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/41c2006638bb42e897be130fef1c5120~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 8"></p>
<blockquote>
<p>Figure 8：图中的时间序列展示了为什么<strong>Leader无法决定对旧任期号的日志条目进行提交</strong>。 (a)中，S1是Leader，部分Follower复制了index&#x3D;2的条目。 (b)中，S1崩溃了，然后S5在任期3中通过S3,S4和自己的选票，赢得了选股，然后从客户端接受了新的条目放在index&#x3D;2处。 (c)中，S5又崩溃了；S1重新启动，选举成功，开始复制日志。这时候，来自任期2的日志已被复制到大多数机器上，但是尚未提交。 (d)中，S1又崩溃了，S5可以重新被选举成功(S2,S3,S4的选票)，然后覆盖它们索引2处的日志。 反之，如果在崩溃之前，S1将自己任期内产生的日志条目复制到大多数机器上，那么就会像(e)中那样，在后面的新的任期中这些条目就会被提交(因为S5此时不能选举成功)。 这样在同一时刻就保证了，之前的旧条目都会被提交。</p>
</blockquote>
<blockquote>
<p>出现这种问题的根本原因就是，(c)中处于任期4的S1提交了任期2的日志 正确的做法应该是等到任期4时大部分节点存储后，同时提交两条记录。 这样，就不会(c)时刻的情形，即任期4的S1不会复制任期2的日志到S3，而是(e)中那样，通过复制-提交任期4的日志而顺带提交任期2的日志，此时即使S1宕机，S5也不能当选成功。(即使任期4没有收到客户端请求，在任期开始时也可以尝试立即提交一条空的日志)</p>
</blockquote>
<p>为了消除图 8 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。 <strong>只有Leader当前任期内的日志条目可以通过计算副本数目被提交</strong>；一旦当前任期的条目通过这种方式被提交，那么<strong>由于日志匹配特性，之前的日志条目也会被间接提交</strong>。 在某些情况下，Leader可以安全的知道一个旧日志条目是否被提交(是否被存储到所有的服务器上)，但是Raft使用了一种更保守的方式。</p>
<p>当Leader复制之前任期的日志时，Raft会为所有的日志<strong>保留原始的任期号</strong>，这在提交规则上产生了额外的复杂性。 在其他的一致性算法中，如果一个新Ledaer需要从之前的任期拷贝日志时，他必须使用当前新的任期号。 Raft使用的方式更容易辨别出日志，因为它可以随着时间和日志的变化维护同一个任期号。 另外，新Leader需要发送的条目更少(和其他算法相比，其他算法中必须在它们commit之前发送更多的冗余日志来为其重新编号)。</p>
<h4 id="5-4-3-Safety-argument-安全性论证"><a href="#5-4-3-Safety-argument-安全性论证" class="headerlink" title="5.4.3 Safety argument 安全性论证"></a>5.4.3 Safety argument 安全性论证</h4><p>在给定完整的Raft算法之后，我们现在可以更加精确的讨论Leader完整性特征(基于9.2节的安全性证明)。我们认为Leader完整性特征不存在，然后我们证明这是矛盾的。 假设任期T的Leader在任期内commit了一条日志，但是这条日志没有存储到未来某个任期的Leader的日志中。假设大于T的最小任期U没有这条日志。</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fbec9adb627645888a88625f0d289b7b~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 9"></p>
<blockquote>
<p>Figure 9：如果S1(任期T的Leader)在它的任期内提交了一条新的日志，然后S5在之后的任期U被选举为Leader，那么至少有一个，例如S3，既拥有来自S1的日志，也拥有来自S5的日志。</p>
</blockquote>
<ol>
<li>在Leader U选举时一定没有那条被提交的日志(Leader不会删除或覆盖日志条目)。</li>
<li>Leader T复制这条日志给了集群的大多数节点，同时U从集群的大多数节点获取的选票。因此至少有一个节点同时接受了来自T的条目，并给U投票了(Figure 9)。这个投票者是矛盾的关键。</li>
<li>这个投票者必须在给U投票前接受来自T的条目；否则它就会拒绝来自T的AppendEntriesRPC(因为此时它的任期大于T)。</li>
<li>投票者给U投票时仍持有这条日志，因为任何中间的Leader都包含该日志条目，Leader从不会删除&#x2F;覆写条目，并且Follower只有在和Leader冲突时才会删除条目。</li>
<li>投票者给U投票时，U的日志必须和自己一样新。这导致了矛盾。</li>
<li>首先，如果投票者和U的最后一条日志的任期号相同，那么U的日志至少和投票者一样长，所以U的日志一定包含了所有投票者的日志。</li>
<li>除此之外，Leader U的最后一条日志的任期号必须比投票者。同时，它也比T大，因此投票者的最后一条日志的任期号至少和T一样大。创建了U的最后一条日志之前Leader一定已经包含了那条被提交的日志。所以根据日志匹配特性，U一定也包含了那条日志，因此矛盾。</li>
<li>这里已经产生了矛盾。因此，所有比T打的Leader一定包含了所有来自T的已被提交的日志。</li>
<li>日志匹配原则保证了未来的Leader也会包含被间接提交的题目，如Figure 8(d)中的索引2。</li>
</ol>
<p>通过领导人完全特性，我们就能证明Figure 3中的状态机安全性，即如果服务器已经在某个给定的索引值应用了日志条目到自己的状态机里，那么其他服务器不会应用不同的日志条目在同一个索引值处。 在一个服务器应用一条日志到自己的状态机中时，它的日志必须和Leader的日志，在该条目和之前的条目相同，并且已经commit。 现在考虑任何一个服务器应用一个指定索引位置的最小任期；日志完全特性保证有更高任期号的领导人会存储相同的日志条目，所以之后的任期中某个索引位置的日志条目也是相同的值。因此状态机安全特性成立。</p>
<p>最后，Raft要求服务器按照日志中索引顺序来应用条目。 和状态机的安全特性结合来看，这意味着所有的服务器会应用相同的日志序列。</p>
<h3 id="5-5-Follower-and-candidate-crashes-跟随者和候选人崩溃"><a href="#5-5-Follower-and-candidate-crashes-跟随者和候选人崩溃" class="headerlink" title="5.5 Follower and candidate crashes 跟随者和候选人崩溃"></a>5.5 Follower and candidate crashes 跟随者和候选人崩溃</h3><p>到目前为止，我们只关注了Leader崩溃的情况。Follower和Candidate的崩溃相比Leader来说，处理起来要简单的多，并且它们的处理方式是相同的。 如果Follower或者Candidate崩溃了，那么之后发送给他的RequestVote和AppendEntriesRPCs将会失败。 Raft中通过重试来处理这种失败；如果崩溃的机器重启的，那么这些RPC就会成功完成。 如果一个服务器在完成一个RPC但还未响应时崩溃了，那么它在重启之后会收到同样的RPC请求。 Raft的RPCs都是幂等的，所以这样的重试没有问题。 例如一个Follower收到一个AppendEntriesRPC，但是他已经包含了这个日志，那他就是直接忽略这个新的请求。</p>
<h3 id="5-6-Timing-and-availability-时间和可用性"><a href="#5-6-Timing-and-availability-时间和可用性" class="headerlink" title="5.6 Timing and availability 时间和可用性"></a>5.6 Timing and availability 时间和可用性</h3><p>我们对Raft的要求之一就是安全性不能依赖于时间：系统不能因为某些事情发生的比预期快&#x2F;慢而生成错误的结果。 然而，可用性(系统可以及时响应客户端)不可避免的要依赖于时间。 例如，如果消息交换比服务器故障间隔时间长，那么Candidate将没有足够的时间来赢得选举；没有一个稳定的Leader，Raft将不能运作。</p>
<p>Leader选举时Raft中对时间要求最关键的方面。Raft可以选举并维持一个稳定的Leadaer，只要系统满足一下要求：</p>
<blockquote>
<p>broadcastTime&lt;&lt;electionTimeout&lt;&lt;MTBFbroadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBFbroadcastTime&lt;&lt;electionTimeout&lt;&lt;MTBF</p>
</blockquote>
<p>在这个不等式中，广播时间是指的是从一个服务器并行的发送RPCs给集群中的其他服务器并接受响应的平均时间；选举超时时间即5.2节介绍的选举的超时时间限制；平故障间隔时间就是对于一台服务器而言，两次故障之间的平均时间。 广播时间必须比选举超时时间小一个量级。这样Leader才能够发送稳定的心跳消息来组织Candidate进入选举状态；通过随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。 选举超时时间应该比平均故障间隔小几个数量级，这样整个系统才能稳定运行。 当Leader崩溃之后，整个系统会大约相当于选举超时的时间内不可用；我们希望这种情况尽可能少出现。</p>
<p>广播时间和平均故障时间是由系统决定的，但是选举超时时间是我们自己决定的。 Raft的RPCs需要接受方将信息持久化的保存到稳定存储总，所以广播时间大约是0.5ms-20ms，区别于存储的技术。 因此，选举超时时间可能需要10ms-500ms。 大多数的服务器的平均故障时间间隔都在几个月甚至更长，很容易满足时间的需求。</p>
<h2 id="6-Cluster-membership-changes-集群成员变更"><a href="#6-Cluster-membership-changes-集群成员变更" class="headerlink" title="6 Cluster membership changes 集群成员变更"></a>6 Cluster membership changes 集群成员变更</h2><p>到目前为止，我们认为集群的配置(参与共识算法的服务器集)是固定的。 实际上，我们需要偶尔改变集群的配置，例如在服务器崩溃时替换它或者改变复制的程度。 尽管这些可以通过暂停整个集群，更新配置，重启来实现，但是这会导致变更时集群不可用。 除此之外，如果存在人工的步骤，这也会有操作失误的风险。 为了避免这些问题，我们决定自动化配置编程的过程并将其整合到Raft共识算法中。</p>
<p>为了保证配置变更的过程是安全的，在变更期间不能存在两个Leader在同一任期同时当选的情况。 不幸的是，任何服务器直接从旧配置变更为新配置的方案都是不安全的。 原子化的一次性变更是不现实的，因此整个集群在变更期间划分成两个独立的主体是有可能的。</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/efa1984ed8c845ac9fa3b0d68be9bba1~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 10"></p>
<blockquote>
<p>Figure 10：直接从一种配置切换到另一种配置时不安全的，因为不同的服务器会在不同的时间切换。例如，集群从3扩增到5台机器。不幸的是，在一个时间点上，两个不同的Leader可以在同一个任期中当选，一个是旧配置的主体选出的(ColdC_{old}Cold)，一个是新配置的主体选出的(CnewC_{new}Cnew)。</p>
</blockquote>
<p>为了确保安全性，配置变更必须使用两阶段方法。 目前有多种两阶段的实现方式。例如有些系统在第一阶段禁用旧配置，使其无法处理客户端请求；然后再第二阶段启动新配置。 在Raft中，集群首先切换到一个称之为<code>joint consensus</code>的过渡配置；一旦共同共识已经commit，系统再切换到新配置。 共同共识结合了新旧配置：</p>
<ul>
<li>日志条目被复制给集群中的所有服务器，无论新旧。</li>
<li>新旧配置的服务器都能竞选Leader。</li>
<li>达成一致(针对选举和提交)需要分别在两种配置上获得大多数的支持。</li>
</ul>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4220df98296644de8bc5b7c0b605d97e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 11"></p>
<blockquote>
<p>Figure 11：配置变更的时间线。虚线表示已经创建的但尚未提交的配置项，实现表示最新提交的配置项。 Leader首先在自己的的日志中创建C-new,C-old的配置条目，然后提交到C-old-new中。然后创建C-new条目并提交给C-new中的大多数，这样就不存在C-new和C-old可以同时决策的时间点。</p>
</blockquote>
<p>共同共识允许独立的服务器在不破坏安全性的前提下，在不同的时间点进行配置转换。此外共同共识允许集群持续响应客户端请求，即使在配置变更中。</p>
<p>集群配置在复制日志中通过特殊的条目来存储和通信；Figure 11展示了配置变更的过程。 当Leader节点接收到一个变更配置从ColdC_{old}Cold到CnewC_{new}Cnew的请求，它会将共同共识的配置(图中的Cold−newC_{old-new}Cold−new)存储为日志条目，并使用前面描述的机制复制该条目。 一旦一个服务器在日志中新增了配置条目，它在未来的决策中都使用该配置(服务器总是使用其日志中的最新配置，无论该条目是否已经提交)。 这意味着Leader会使用Cold,newC_{old,new}Cold,new的规则来决定Cold,newC_{old, new}Cold,new的体质条目何时commit。 如果Leader崩溃了，一个新的Leader会在ColdC_{old}Cold或者Cold,newC_{old,new}Cold,new的配置下被选举，这取决于赢得选举的Candidate是否已经收到了Cold,newC_{old,new}Cold,new配置。 在任何情况下，CnewC_{new}Cnew配置在这一时期都不能做出单方面的决策。</p>
<p>一旦Cold,newC_{old,new}Cold,new配置已经提交，未经另一个配置的允许，CnewC_{new}Cnew和ColdC_{old}Cold都不能做出决策，并且Leader完全特性保证了只有拥有Cold,newC_{old,new}Cold,new的节点才能被选举为Leader。 这个时候，Leader创建一条关于CnewC_{new}Cnew配置的日志条目并复制给其他机器就是安全的。 同样，该配置将会在每台服务器上生效。 当新配置已经在CnewC_{new}Cnew的规则下提交，旧配置就不重要了，同时未应用新配置的服务器就可以被关闭了。 Figure 11中，ColdC_{old}Cold和CnewC_{new}Cnew没有任何机会做出单方决策；这确保了安全性。</p>
<p>重新配置还有三个问题需要解决。 <strong>第一个问题，新的服务器可能最初没有存储任何条目</strong>。如果以这种状态加入到集群中，那么他们需要相当长的时间来同步，而且在这段时间里不能提交新的条目。 为了便面这种间隔，Raft在配置变更之前引入一个额外的阶段，在这个阶段，新的服务器以没有投票权的身份加入到集群(Leader拷贝日志给它们，但是不考虑它们时大多数)。 一旦新的服务器和其他的机器同步了，就可以向上述一样处理配置变更了。 <strong>第二个问题，集群Leader可能不是新配置中的一部分</strong>。在这种情况下，Leader一旦提交完CnewC_{new}Cnew日志就会变成Follower状态。这意味着有一段时间(提交CnewC_{new}Cnew时)Leader管理着集群但是不包括它自己；他复制日志但不将自己作为大多数之一。 Leader变更将会在CnewC_{new}Cnew提交时发生，因为这是新配置可以独立工作的最早时间(总是能在新配置下选出Leader)。 在此之前，只能从旧配置下选举Leader。</p>
<p><strong>第三个问题，被移除的服务器（不在CnewC_{new}Cnew中的服务器）可能会破坏集群</strong>。这些服务器将不会收到心跳，所以当选举超时时，它们就会开始新的一轮选举。 它们会发送RequestVote RPCs(携带新的任期号)，这会导致当前的Leader回退为Follower状态(因为集群中没有收到CnewC_{new}Cnew的节点开启了新的任期号)。 新的Leader最终被选出来，但是被移除的服务器会再次超时，然后再次重复这个过程，导致可用性很差。</p>
<p>为了避免这个问题，当服务器确认当前的Leader存在时，会忽略RequestVoteRPCs请求。 准确来说，当服务器接在当前的最小选举超时时间内收到一个RequestVoteRPC，他不会更新任期号或者投出选票。 这不会影响正常的选举，每个服务器在开始一次选举之前，必须至少等待一个最小选举超时时间。 然而，这有利于避免被移除的服务器扰乱集群：如果Leader能够和集群保持心跳，那么它就不会被更大的任期号废黜。</p>
<h2 id="7-Log-campaction-日志压缩"><a href="#7-Log-campaction-日志压缩" class="headerlink" title="7 Log campaction 日志压缩"></a>7 Log campaction 日志压缩</h2><p>Raft的日志在正常的运作中会不断增长，但是在实际体统中，日志是不能无限长增长的。 随着日志增长，它会占用更多的空间，花费更多的时间重置。 如果没有某些机制来清除日志中堆积的陈旧信息，那么最终会导致可用性出现问题。</p>
<p>快照时最简单的压缩方式。 在快照中，整个系统的状态都以快照的形式写入到稳定存储中，然后在这之前的所有日志全部清除。快照在Chubby和Zookeeper中均有使用，接下来会介绍Raft中的快照技术。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7ff71ac1e71845df843c40a0e5d75291~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 12"></p>
<blockquote>
<p>Figure 12：一个服务器用一个新快照来替代日志中的已提交条目(index1~5)，快照存储了当前的状态(x，y)。日志的最后索引位置和任期号都包含在其中。</p>
</blockquote>
<p>增量压缩，例如日志清理或者日志结构合并树，都是可行的。 这些方法每次只对一小部分的数据操作，这样可以分散压缩的负载压力。 它们先选择一个已经累积了许多被删除和覆盖的对象的数据区域，然后将该区域的存活对象重写地更加紧凑，并释放这块区域。 和简单操作整个数据集的快照相比，这需要更复杂的额外机制。 状态机可以实现LSM树，使用和快照相同的接口，但是日志清除可能需要修改Raft了。</p>
<p>Figure12展示了Raft中快照的基本思想。 每个服务器可以独立创建快照，只包括已经被提交的日志。主要工作是将状态机的当前状态写入快照中。 Raft也包含一些少量的元数据在快照中：<code>last included index</code>指的是被快照取代的最后条目在日志中的索引值(状态机最后应用的日志)，<code>last included term</code>指的是该条目的任期号。保留这些是为了支持快照后紧接着的第一个AppendEntriesRPCs的一致性检查，因为这个条目需要前一个条目的索引值和日期号。 为了支持成员变更，快照中也将最后一次配置作为最后一个条目存储下来。 一旦服务器完成快照的写入，它可以删除最后索引之前的所有日志和快照。</p>
<p>尽管通常服务器都是独立的创建快照，Leader必须偶尔发送快照给一些落后的Follower。 这通常发生在Leader已经丢弃了它需要发送给Follower的下一个日志条目时。 幸运的是，这种情况不是常规操作：一个与Leader保持同步的Follower通常持有这个条目。 然而一个运行缓慢的Follower或者刚加入集群的服务器将不会持有这个条目。 这时让这个Follower更新到最新状态的方式就是通过网络将快照发送给它。</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c48098945003447f8a56c7444f80663b~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 13"></p>
<blockquote>
<p>Figure 13：InstallSnapshotRPC的概要。快照被分割成小块进行传输；每个分块都给予Follower生命的信号，所以Follower可以重置选举超时计时器。</p>
</blockquote>
<p>Leader使用一种名为InstallSnapshotRPC的方式来发送快照给太落后的Follower；见Figure 13。当Follower通过这种RPC接收到快照时，他必须自己决定对于它的现存日志如何处理。 通常快照会包括接受者的日志中不存在的信息。在这种情况下，Follower会丢弃其整个日志；全部被快照取代，并且可能包含于快照冲突的未提交条目。 如果接受的快照是自己日志的前缀部分(由于网络重传或者错误)，那么快照包含的条目将会被清除，但是快照后的条目仍然有效，会被保留。</p>
<p>这种快照的方式违背了Raft的强Leader原则，因为Follower可以在Leader不知道的情况下创建快照。 但是我们认为这种背离是正确的。 Leader的存在，是为了解决在达成共识时带来的冲突，但是创建快照时，共识已经达成，已经不存在冲突了。 数据仍然是从Leader流向Follower，只不过Follower重新组织了数据而已。</p>
<p>我们思考过另一种基于Leader的方法，即只有Leader创建快照，然后发送给所有的Follower。 但是，这样做会有两个弊端。 第一，发送快照会浪费网络带宽并且延缓快照处理的时间按。每一个Follwer既然已经拥有了生产快照的信息，显然在自己本地创建快照要比通过网络接受更加有效。 第二，Leader的实现将会更加复杂。例如，Leader需要发送快照的同时并行请求将新的日志条目发送给Follower，这样才不会阻塞新的客户端请求。</p>
<p>还有两个问题影响了快照的性能。 第一个，服务器必须决定何时创建快照。如果快照创建过于频繁，这会浪费大量的磁盘带宽；如果快照创建频率太低，它就要承受耗尽存储的风险，同时增加了日志重建的时间。一个简单的策略就是在日志达到以字节为单位的固定大小时就创建快照。如果将该大小设置为明显大于快照的预期大小，那么用于快照的磁盘带宽开销将很小。 第二个，就是写入快照通常需要花费一段不少的时间，而且我们希望这不要影响正常的操作。解决的方式时使用写入时复制技术(copy-on-write)，这样可以接受新的更新而不会影响正在写入的快照。例如，使用函数式数据结构的状态机天然支持这样的功能。此外，操作系统的copy-on-write技术(如Linux中的fork)可以被用于创建完整的状态机内存快照(我们就是这样实现的)。</p>
<h2 id="8-Client-interaction-客户端交互"><a href="#8-Client-interaction-客户端交互" class="headerlink" title="8 Client interaction 客户端交互"></a>8 Client interaction 客户端交互</h2><p>这一节将介绍客户端是如何于Raft交互的，包括客户端如何发现集群Leader和Raft如何支持线性化的语义的。 这些问题对于所有的基于共识的系统都是存在的，并且Raft 的解决方式和其他系统相似。</p>
<p>Raft的客户端发送所有的请求给Leader。当客户端启动时，它会随机挑选一个服务器进行通信。若果客户端第一次挑选的服务器不是Leader，那么这个服务器会拒绝请求，并提供它最新的Leader信息(AppendEntriesRPC中有)。如果Leader已经崩溃了，那么请求会超时；客户端之后会重试。</p>
<p>Raft的目标是实现线性化的语义(每一次操作立即执行，只执行一次，在他调用和收到回复之间)。但是，如上述，Raft可能会多次执行同一条指令：例如，Leader在提交日志条目后，在响应客户端前崩溃了，那么客户端就会和新的Leader重试这条命令，导致这条命令吧被再次执行。 解决方式就是客户端对于每条指令赋予一个唯一的序列号。然后状态机跟踪每条指定最新的序列号和其响应。如果收到一条已被执行的序列号，那么可以直接返回结果。</p>
<p>只读的操作可以不需要记录日志而直接处理。但是在没有额外措施的情况下，这可能会有风险读取脏数据，因为响应客户端请求的Leader可能不知道它已经不再是Leader了。 线性化的读取一定不能返回脏数据，Raft需要两个额外措施来保证不使用日志的情况下实现这一点。 首先Leader必须有关于已经提交的日志的最新信息，但是在它任期刚开始时，它可能不知道哪些信息时已被提交的。因此，在它任期开始时，它会提交一条空的日志。(日志匹配机制) 第二，Leader处理只读的请求时必须检查自己是否已被废黜(产生了新Leader)。Raft通过让Leader在响应只读请求直接先和集群中的大多数节点交换一次心跳信息来应对这个问题。或者，Leader可以依靠心跳机制来提供一种租约的机制，但是这种方式依赖时间来保证安全性(它假设时间误差是有界的)。</p>
<h2 id="9-Implementation-and-evaluation-算法实现和评估"><a href="#9-Implementation-and-evaluation-算法实现和评估" class="headerlink" title="9 Implementation and evaluation 算法实现和评估"></a>9 Implementation and evaluation 算法实现和评估</h2><p>我们已经为RAMCloud实现了Raft算法作为存储配置信息的复制状态机的一部分，并且帮助RAMCloud协调故障转移。 这个实现大约2000行左右的C++代码，其中不包括测试，注释和空行。 这些代码是开源的。同时也有大约25个独立的第三方开源实现(针对不同的场景)。当然，很多企业已经部署了基于Raft的系统。</p>
<p>这一节会从三个方面评估Raft：可理解性，正确性，性能。</p>
<h3 id="9-1-Understandability-可理解性"><a href="#9-1-Understandability-可理解性" class="headerlink" title="9.1 Understandability 可理解性"></a>9.1 Understandability 可理解性</h3><p>为了和 Paxos 比较 Raft 算法的可理解性，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。 我们分别安排了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文的所有内容除了日志压缩；Paxos 讲课包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如Leader选举）。 小测验测试一些对算法的基本理解和解释一些边角的示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。 大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者从第一部分的算法学习中获得的表现和经验的差异。 我们计算参加人员的每一个小测验的得分来看参与者是否在 Raft 算法上更加容易理解。</p>
<p>我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些 Paxos 的经验，并且 Paxos 的视频要长 14%。 如Table 1总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。</p>
<table>
<thead>
<tr>
<th>关心</th>
<th>缓和偏见采取的手段</th>
<th>可供查看的材料</th>
</tr>
</thead>
<tbody><tr>
<td>相同的讲课质量</td>
<td>两者使用同一个讲师。Paxos 使用的是现在很多大学里经常使用的。Paxos 会长 14%。</td>
<td>视频</td>
</tr>
<tr>
<td>相同的测验难度</td>
<td>问题以难度分组，在两个测验里成对出现。</td>
<td>小测验</td>
</tr>
<tr>
<td>公平评分</td>
<td>使用评价量规。随机顺序打分，两个测验交替进行。</td>
<td>评价量规（rubric）</td>
</tr>
</tbody></table>
<blockquote>
<p>Table 1：考虑到可能会存在的偏见，对于每种情况的解决方法，和相应的材料。</p>
</blockquote>
<p>参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；Figure 14 展示了每个参与者的得分。 配置t-检验（又称student‘s t-test）表明，在 95% 的可信度下，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/84f34635b55a4932b33dbf81471ad013~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 14"></p>
<blockquote>
<p>Figure 14：一个散点图表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩。在对角线之上的点表示在 Raft 获得了更高分数的学生。</p>
</blockquote>
<p>我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型预测，对小测验的选择会产生 12.5 分的差别。 这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。 但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft的得分低了6.3分; 虽然我们不知道为什么，这似乎在统计上是有意义的。</p>
<p>我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在Figure 15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。 但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/48f0c2f87b3e444497378ead8a84f69d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 15"></p>
<blockquote>
<p>Figure 15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。</p>
</blockquote>
<h3 id="9-2-Correctness-正确性"><a href="#9-2-Correctness-正确性" class="headerlink" title="9.2 Correctness 正确性"></a>9.2 Correctness 正确性</h3><p>在第 5 节，我们已经制定了正式的规范，和对一致性机制的安全性证明。这个正式规范使用 TLA+ 规范语言使图Figure 2中总结的信息非常清晰。 它长约400行，并作为证明的主题。同时对于任何想实现 Raft 的人也是十分有用的。 我们通过 TLA 证明系统非常机械的证明了日志完全特性。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明规范的类型安全）。而且，我们已经写了一个非正式的证明关于状态机安全性是完备的，并且是相当清晰的（大约 3500 个词）。</p>
<h3 id="9-3-Performance-性能"><a href="#9-3-Performance-性能" class="headerlink" title="9.3 Performance 性能"></a>9.3 Performance 性能</h3><p>Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从Leader到集群大多数机器的消息）就达成了这个目的。 同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。</p>
<p>我们使用我们自己的 Raft 实现来衡量 Raft 的Leader选举的性能并且回答两个问题。首先，Leader选举的过程收敛是否快速？第二，在Leader宕机之后，最小的系统宕机时间是多久？</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/caaf299f5d814fc0898dc987ba413308~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp" alt="Figure 16"></p>
<blockquote>
<p>Figure 16：发现并替换一个已经崩溃的Leader的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小选举超时时间。 每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。 例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。</p>
</blockquote>
<p>为了衡量Leader选举，我们反复的使一个拥有五个节点的服务器集群的Leader宕机，并计算需要多久才能发现Leader已经宕机并选出一个新的Leader（见Figure 16）。 为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些Candidate是没有成为Leader的资格的。 另外，为了促成选票瓜分的情况，我们的测试脚本在终止Leader之前同步的发送了一次心跳广播（这大约和Leader在崩溃前复制一个新的日志给其他机器很像）。Leader均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。</p>
<p>Figure 16 中上面的图表明，<strong>只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。</strong> 在没有随机化的情况下，在我们的测试里，选举过程往往都需要花费超过 10 秒钟由于太多的选票瓜分的情况。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。 增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。</p>
<p>Figure 16 中下面的图显示，<strong>通过减少选举超时时间可以减少系统的宕机时间</strong>。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的Leader（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求： 在选举新Leader之前，Leader就很难发送完心跳包。这会导致没有意义的Leader改变并降低了系统整体的可用性。 我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的Leader改变，而且依然提供不错的可用性。</p>
<h2 id="10-Related-work-相关工作"><a href="#10-Related-work-相关工作" class="headerlink" title="10 Related work 相关工作"></a>10 Related work 相关工作</h2><p>已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：</p>
<ul>
<li>Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰。</li>
<li>关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。</li>
<li>实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 着实有着很大的差别。</li>
<li>Paxos 可以应用的性能优化。</li>
<li>Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。</li>
</ul>
<p>Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用Leader选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了Leader身上。 这样就可以使得算法更加容易理解。例如，在 Paxos 中，Leader选举和基本的一致性协议是正交的：Leader选举仅仅是性能优化的手段，而且不是一致性所必须要求的。 但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对Leader选举的独立的机制。 相比较而言，Raft 就直接将Leader选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。</p>
<p>像 Raft 一样，VR 和 ZooKeeper 也是基于Leader的，因此他们也拥有一些 Raft 的优点。 但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非Leader的功能。例如，Raft 中日志条目都遵循着从Leader发送给其他人这一个方向：附加条目 RPC 是向外发送的。 在 VR 中，日志条目的流动是双向的（Leader可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。</p>
<p>和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。 VR 和 ZooKeeper 都分别定义了 10 种不同的消息类型，相对的，Raft 只有 4 种消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。 另外，VR 和 ZooKeeper 都在Leader改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。</p>
<p>Raft 的强Leader模型简化了整个算法，但是同时也排斥了一些性能优化的方法。 例如，平等主义 Paxos （EPaxos）在某些没有Leader的情况下可以达到很高的性能。 平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。 然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。 因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。</p>
<p>一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。 我们选择使用共同一致的方法因为他对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。 Lamport 的基于 α 的方法之所以没有被 Raft 选择是因为它假设在没有Leader的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较的，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。 Raft 的方法同时也需要更少的额外机制来实现，和 VR、SMART 比较而言。</p>
<h2 id="11-Conclusion-总结"><a href="#11-Conclusion-总结" class="headerlink" title="11 Conclusion 总结"></a>11 Conclusion 总结</h2><p>算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。 在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。 除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。</p>
<p>在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。 我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。 把可理解性作为设计的目标改变了我们设计 Raft 的方式；随着设计的进展，我们发现自己重复使用了一些技术，比如分解问题和简化状态空间。 这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。</p>
<h2 id="12-Acknowledgments-致谢"><a href="#12-Acknowledgments-致谢" class="headerlink" title="12 Acknowledgments 致谢"></a>12 Acknowledgments 致谢</h2><p>这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazieres，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。</p>
<blockquote>
<p>PS：第9-12节的翻译直接引用了<a href="https://link.juejin.cn/?target=https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md%238-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BA%A4%E4%BA%92">github.com&#x2F;maemual&#x2F;raf…</a></p>
</blockquote>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>鹤染</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://starry54.github.io/2023/08/ba8fdbe5abd1.html">https://starry54.github.io/2023/08/ba8fdbe5abd1.html</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2023/08/48e2ec765c40.html">绝对=>相对</a>
            
            
            <a class="next" rel="next" href="/2023/08/674a10417a26.html">MapReduce</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 鹤染 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>